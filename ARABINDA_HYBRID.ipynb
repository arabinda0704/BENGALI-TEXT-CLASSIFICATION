{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def Preprocess_Reference_text(text):\n",
    "    \n",
    "    #text=punc_remove(text)\n",
    "    #print(text)\n",
    "    text=text.replace(\"-\",\" \")\n",
    "    text=text.replace(\",\",\" \")\n",
    "    text=text.replace(\"।\",\" \")\n",
    "    text=text.replace(\"\\n\",\" \")\n",
    "    text=text.replace(\"!\",\" \")\n",
    "    text=text.replace(\":\",\" \")\n",
    "    text=text.replace(\"‘\",\" \")\n",
    "    text=text.replace(\"’\",\" \")\n",
    "    \n",
    "    tokens=[]\n",
    "    tokens=text.split(\" \")\n",
    "  \n",
    "    \n",
    "    # convert to lower case\n",
    "    #tokens = [w.lower() for w in tokens]\n",
    "                \n",
    "    '''# remove punctuation from each word\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    #table = str.maketrans(''। '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    #print(stripped)\n",
    "    #get length of documents\n",
    "    #print('Number of words in reference summary :', len(stripped))\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #words = [w for w in stripped if not w.lower in stop_words]\"\"\"\n",
    "    words=[]\n",
    "    for w in stripped:\n",
    "        if w.lower not in stop_words:\n",
    "            words.append(w) \n",
    "    '''\n",
    "    import string\n",
    "    #words = [w for w in stripped if not w.lower in stop_words]        \n",
    "    words=[]\n",
    "    RootDirectory=\"C:/Users/ARABINDA DAS/OneDrive/Desktop/LSI/LSI/\"\n",
    "    with open(RootDirectory+\"bengali_stoplist_sort1.txt\",\"r\",encoding='utf-8') as f:\n",
    "            \n",
    "            text2 = f.read()\n",
    "    \n",
    "    tokens_stop=text2.replace(\"\\n\",\" \")\n",
    "    tokens_stop_words=[]\n",
    "    tokens_stop_words=tokens_stop.split(\" \")\n",
    "    for w in tokens:\n",
    "        if w not in tokens_stop_words:\n",
    "            words.append(w) \n",
    "    #print(words)\n",
    "    return words\n",
    "    '''ref_sum=[]            \n",
    "    for j in  tokens:\n",
    "      \n",
    "        # checking whether the char is punctuation.\n",
    "        if j not in string.punctuation:\n",
    "            if j != \"P\":\n",
    "          \n",
    "                # Printing the punctuation values \n",
    "                #print(\"Punctuation: \" + i)\n",
    "                if \"-\" in j:\n",
    "                    tokens=j.split(\"-\")\n",
    "                    for t in tokens:\n",
    "                        ref_sum.append(t)\n",
    "                ref_sum.append(j)  \n",
    "    \n",
    "    return ref_sum''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adcd318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def get_stem_words(words):\n",
    "    # stemming of words\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    #porter = PorterStemmer()\n",
    "    ps = PorterStemmer()\n",
    "    stemmed = [ps.stem(w) for w in words]\n",
    "    \n",
    "    #print(stemmed)\n",
    "    return stemmed\n",
    "def get_tf_val(stemmed):\n",
    "    from nltk.probability import FreqDist\n",
    "    data_analysis = nltk.FreqDist(stemmed)\n",
    " \n",
    "    # Let's take the specific words only if their frequency is greater than 0.\n",
    "    filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 0])\n",
    "    return filter_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc81e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY=RootDirectory=\"C:/Users/ARABINDA DAS/OneDrive/Desktop/LSI/LSI/\"\n",
    "folder_read=\"data_new\"\n",
    "\n",
    "file_counter=0\n",
    "total_words=[]\n",
    "corpusDoc_length_dict={}\n",
    "corpus_fp_sum_dict={}\n",
    "corpus_fp_dict={}\n",
    "corpus_tf_dict={}\n",
    "corpus_tf_sum_dict={}\n",
    "word_dict={} #contain doc wise word list\n",
    "DF={}\n",
    "corpus_doc_length_dict={}\n",
    "file_in_cluster_dict={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "file_list = os.listdir(RootDirectory+folder_read+\"/\")\n",
    "count = 0\n",
    "file_list=list(file_list)\n",
    "# print(file_list)\n",
    "category=[]\n",
    "for i in range (0,len(file_list)):\n",
    "    #print(len(file_list))\n",
    "    #print (file_list[i])\n",
    "    path=file_list[i]\n",
    "    #print(path)\n",
    "    path=str(path)\n",
    "    cluster_name=path\n",
    "    file_inDir=os.listdir(RootDirectory+folder_read+\"/\"+path+\"/\")\n",
    "    s=int(file_list[i].replace('f',''))\n",
    "    if s>=0 and s<=49:\n",
    "        category.append(\"Agriculture\")\n",
    "    elif s>=50 and s<=99:\n",
    "        category.append(\"Banking\")\n",
    "    elif s>=100 and s<=149:\n",
    "        category.append(\"Business\")\n",
    "    elif s>=150 and s<=191:\n",
    "        category.append(\"Caste\")\n",
    "    elif s>=192 and s<=241:\n",
    "        category.append(\"Cinema\")\n",
    "    elif s>=242 and s<=291:\n",
    "        category.append(\"Computer\")\n",
    "    elif s>=292 and s<=341:\n",
    "        category.append(\"Cricket\")\n",
    "    elif s>=342 and s<=391:\n",
    "        category.append(\"Crime\")\n",
    "    elif s>=392 and s<=441:\n",
    "        category.append(\"Defence\")\n",
    "    elif s>=442 and s<=491:\n",
    "        category.append(\"Economy\")\n",
    "    elif s>=492 and s<=541:\n",
    "        category.append(\"Education\")\n",
    "    elif s>=542 and s<=591:\n",
    "        category.append(\"election\")\n",
    "    elif s>=592 and s<=641:\n",
    "        category.append(\"Electronics\")\n",
    "    elif s>=642 and s<=676:\n",
    "        category.append(\"Energy\")\n",
    "    elif s>=677 and s<=730:\n",
    "        category.append(\"Entertainment\")\n",
    "    elif s>=731 and s<=780:\n",
    "        category.append(\"Environment\")\n",
    "    elif s>=781 and s<=830:\n",
    "        category.append(\"Family issues\")\n",
    "    elif s>=831 and s<=880:\n",
    "        category.append(\"Finance\")\n",
    "    elif s>=881 and s<=930:\n",
    "        category.append(\"Football\")\n",
    "    elif s>=931 and s<=980:\n",
    "        category.append(\"Government_Operations\")\n",
    "    elif s>=981 and s<=1030:\n",
    "        category.append(\"Health\")\n",
    "    elif s>=1031 and s<=1080:\n",
    "        category.append(\"Labor_and_Employment\")\n",
    "    elif s>=1081 and s<=1130:\n",
    "        category.append(\"Law\")\n",
    "    elif s>=1131 and s<=1180:\n",
    "        category.append(\"Miscellaneous\")\n",
    "    elif s>=1181 and s<=1230:\n",
    "        category.append(\"Music\")\n",
    "    elif s>=1231 and s<=1280:\n",
    "        category.append(\"Politics\")\n",
    "    elif s>=1281 and s<=1304:\n",
    "        category.append(\"Public_lands_and_water_management\")\n",
    "    elif s>=1305 and s<=1354:\n",
    "        category.append(\"Religion\")\n",
    "    elif s>=1355 and s<=1404:\n",
    "        category.append(\"Science\")\n",
    "    elif s>=1405 and s<=1415:\n",
    "        category.append(\"Social_welfare\")\n",
    "    elif s>=1416 and s<=1461:\n",
    "        category.append(\"Space\")\n",
    "    elif s>=1462 and s<=1511:\n",
    "        category.append(\"Sports_other_than_football_and_cricket\")\n",
    "    elif s>=1512 and s<=1561:\n",
    "        category.append(\"Technology\")\n",
    "    elif s>=1562 and s<=1605:\n",
    "        category.append(\"Transportation\")\n",
    "    elif s>=1606 and s<=1655:\n",
    "        category.append(\"Travel\")\n",
    "    elif s>=1656 and s<=1705:\n",
    "        category.append(\"Weather\")\n",
    "    elif s>=1706 and s<=1755:\n",
    "        category.append(\"World_and_international\")\n",
    "    file_inDir=list(file_inDir)\n",
    "\n",
    "    #s=int()\n",
    "\n",
    "    cluster_fp_sum_dict={}# cluster wise dictionary of fractional presence of word sum value \n",
    "    cluster_fp_dict={}\n",
    "    cluster_tf_sum_dict={}\n",
    "    cluster_fp_sum_dict={}\n",
    "    TF_DICT={} #contains doc wise tf dictionary\n",
    "    FP_DICT={} #contains doc wise fractional presence of words in document\n",
    "    Doc_length_dict={}\n",
    "    file_count_in_cluster=0\n",
    "    for m in range (0,len(file_inDir)):\n",
    "        file_counter=file_counter+1\n",
    "        file_count_in_cluster+=1\n",
    "        #print(\"file number  :\",file_counter)\n",
    "        file_name=file_inDir[m]\n",
    "        flag=False\n",
    "        \n",
    "        #doc1=sum_of_word_vec(path,file_name)\n",
    "        with open(RootDirectory+folder_read+\"/\"+path+\"/\"+file_name,\"r\",encoding='utf-8') as f:\n",
    "            \n",
    "            text = f.read()\n",
    "            #print(path, file_name)\n",
    "            \n",
    "            \n",
    "            #Words=Preprocess_Reference_text(text,stop_words)\n",
    "            Words=Preprocess_Reference_text(text)\n",
    "            #print(Words)\n",
    "            stemmed_words=get_stem_words(Words)\n",
    "            tf_dict=get_tf_val(stemmed_words)\n",
    "            if file_name ==\"AP901121-0206\":\n",
    "                print(text)\n",
    "                print(\"Words\",Words)\n",
    "            #print(summary)\n",
    "            f.close()\n",
    "            \n",
    "            Doc_tf_dict={}\n",
    "            Doc_fp_dict={}# fp=fracctional present\n",
    "            words1=[]\n",
    "            for w in set(Words):\n",
    "                #print(w, \" : \", ps.stem(w))\n",
    "                stem_word=ps.stem(w)\n",
    "                #doc_fp_val=0   \n",
    "                if stem_word in stemmed_words:\n",
    "                    tf=tf_dict[stem_word]\n",
    "                    doc_fp_val=tf/len(Words)\n",
    "                    Doc_tf_dict[w]=tf\n",
    "                    Doc_fp_dict[w]=doc_fp_val\n",
    "                    if w in cluster_tf_sum_dict:\n",
    "                        prev_tf_val=cluster_tf_sum_dict.get(w)\n",
    "                        if w==\"Hurricane\":\n",
    "                            print(cluster_name, file_name,\"word \",w,\"prev_tf_val\",prev_tf_val)\n",
    "                        #print(\"prev_tf val \",prev_tf_val)\n",
    "                        cluster_tf_sum_dict[w]=tf+prev_tf_val\n",
    "                        new_tf_val=cluster_tf_sum_dict.get(w)\n",
    "                        #print(\"new tf val\",new_tf_val)\n",
    "\n",
    "                    else:\n",
    "                        #print(w)\n",
    "                        #print(\"tf  \",tf)\n",
    "                        cluster_tf_sum_dict[w]=tf\n",
    "                    if w in cluster_fp_sum_dict:\n",
    "                        #print(w)\n",
    "                        prev_fp_val=cluster_fp_sum_dict.get(w)\n",
    "                        print(prev_fp_val)\n",
    "                        cluster_fp_sum_dict[w]=doc_fp_val+prev_fp_val\n",
    "                        new_fp_val=cluster_fp_sum_dict.get(w)\n",
    "                        #print(\"new\",new_fp_val)\n",
    "\n",
    "                    else:\n",
    "                        #print(w)\n",
    "                        #print(doc_fp_val)\n",
    "                        cluster_fp_sum_dict[w]=doc_fp_val    \n",
    "            TF_DICT[file_name]=Doc_tf_dict \n",
    "            FP_DICT[file_name]=Doc_fp_dict\n",
    "            \n",
    "    corpus_fp_sum_dict[cluster_name]=cluster_fp_sum_dict\n",
    "    corpus_tf_sum_dict[cluster_name]=cluster_tf_sum_dict\n",
    "    corpus_fp_dict[cluster_name]=FP_DICT\n",
    "    file_in_cluster_dict[cluster_name]=file_count_in_cluster\n",
    "    #corpus_fp_dict[cluster_name]=FP_DICT\n",
    "    corpus_tf_dict[cluster_name]=TF_DICT\n",
    "    corpus_doc_length_dict[cluster_name]=Doc_length_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30ddbb",
   "metadata": {},
   "source": [
    "# Load idf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11dbff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idfDict={}\n",
    "# sub_string=\"##8.41\"#if not present in idf dict then write 8.18\n",
    "# with open(RootDirectory+\"IDF_value_sort.txt\",\"r\",encoding='utf-8') as f3:\n",
    "#     for text1 in f3:\n",
    "#         if sub_string not in  text1:\n",
    "#             values = text1.split(\"<\")\n",
    "#             word = values[0]\n",
    "#             d_word = word #this line for decode byte object to string object\n",
    "#             idf_val=values[1].rstrip(\"\\n\")\n",
    "#             #print(\"d_word\",d_word,\"idf_val\",idf_val)\n",
    "#             idfDict[d_word] = float(idf_val)\n",
    "#     f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73cefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(RootDirectory+\"corpus_tf_sum_dict.json\", \"w\") as outfile3:  \n",
    "    json.dump(corpus_tf_sum_dict, outfile3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9dd52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "corpus_tf_dictt={}\n",
    "with open(RootDirectory+\"corpus_tf_sum_dict.json\", \"r\") as outfile3:\n",
    "    data1=json.load(outfile3)\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "data = pd.DataFrame.from_dict(data1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d054bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8502a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #new.head(47500)\n",
    "# dataframe = pd.DataFrame(data.data,columns = data.feature_names)\n",
    " \n",
    "# # Convert entire data frame as string and print\n",
    "# print(dataframe.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e929b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# reading JSON file\n",
    "df = pd.read_json(\"C:/Users/ARABINDA DAS/OneDrive/Desktop/LSI/LSI/corpus_tf_sum_dict.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying sample output\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee38177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(166)\n",
    "import numpy as np\n",
    "\n",
    "df = df.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f7ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# df.head()\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be59264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "jsonString = json.dumps(corpus_tf_sum_dict)\n",
    "jsonFile = open(\"data.json\", \"w\")\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01459633",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"data.json\")\n",
    "data=json.load(f)\n",
    "word_list=[]\n",
    "for i in data.keys():\n",
    "    #print(i)\n",
    "    dict1=data.get(i)\n",
    "    keyword=dict1.keys()\n",
    "    #print(dict1)\n",
    "    #print(keyword)\n",
    "    for word in keyword:\n",
    "        word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word_list)\n",
    "# for i in word_list:\n",
    "#     #print(i)\n",
    "#     if i not in idfDict.keys():\n",
    "       \n",
    "#         idfDict[i]=8.18\n",
    "#     print(i, idfDict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose=np.transpose(df)\n",
    "print(len(transpose))\n",
    "print(transpose)\n",
    "transpose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(matrix):\n",
    "    num_documents = matrix.shape[0]\n",
    "    document_frequency = np.sum(matrix > 0, axis=0)\n",
    "    idf = np.log(num_documents / (document_frequency + 1))  # Add 1 to avoid division by zero\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf81a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = calculate_idf(transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c9896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d38fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idfD={}\n",
    "for key in idf.keys():\n",
    "    # print(key,idf.get(key))\n",
    "    idfD[key]=idf.get(key)\n",
    "print(idfD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa49831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(RootDirectory+\"idf.json\", \"w\") as outfile3:  \n",
    "    json.dump(idfD, outfile3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5347ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"data.json\")\n",
    "\n",
    "tf_idf_dict={}\n",
    "tf_idf_dict_all={}\n",
    "\n",
    "data=json.load(f)\n",
    "for i in data.keys():\n",
    "    tf_idf_val_dict={}\n",
    "    print(i)\n",
    "    dict1=data.get(i)\n",
    "    keyword=dict1.keys()\n",
    "    #print(dict1)\n",
    "    #print(keyword)\n",
    "    for word in keyword:\n",
    "        tf_val=dict1.get(word)\n",
    "        idf_val=idfD[word]\n",
    "        tf_idf_val=(tf_val*idf_val)\n",
    "        print(word,tf_val,idf_val,tf_idf_val)\n",
    "        tf_idf_val_dict[word]=tf_idf_val\n",
    "    tf_idf_dict[i]=tf_idf_val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=open(\"data.json\")\n",
    "\n",
    "# tf_idf_dict={}\n",
    "# tf_idf_dict_all={}\n",
    "\n",
    "# data=json.load(f)\n",
    "# for i in data.keys():\n",
    "#     tf_idf_val_dict={}\n",
    "#     print(i)\n",
    "#     dict1=data.get(i)\n",
    "#     keyword=dict1.keys()\n",
    "#     #print(dict1)\n",
    "#     #print(keyword)\n",
    "#     for word in keyword:\n",
    "#         #print(word)\n",
    "#         tf_val=dict1.get(word)\n",
    "#         idf_val=idfDict[word]\n",
    "\n",
    "#         # print(tf_val)\n",
    "#         # print( idf_val)\n",
    "#         tf_idf_val=(tf_val*idf_val)\n",
    "#         print(word,tf_val,idf_val,tf_idf_val)\n",
    "#         tf_idf_val_dict[word]=tf_idf_val\n",
    "#     tf_idf_dict[i]=tf_idf_val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f69c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(RootDirectory+\"tf_idf_dict.json\", \"w\") as outfile3:  \n",
    "    json.dump(tf_idf_dict, outfile3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dict[\"f0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# reading JSON file\n",
    "df1 = pd.read_json(\"C:/Users/ARABINDA DAS/OneDrive/Desktop/LSI/LSI/tf_idf_dict.json\")\n",
    "\n",
    "# displaying sample output\n",
    "df1 = df1.replace(np.nan, 0)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df1.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefab92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7413a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df1.to_numpy()\n",
    " \n",
    "print('\\nNumpy Array\\n----------\\n', arr)\n",
    "print(type(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded1748",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numpy Array Datatype :', arr.dtype)\n",
    "print(len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans=np.transpose(arr)\n",
    "print(len(trans))\n",
    "print(trans)\n",
    "trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f23aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import array\n",
    "# from numpy import diag\n",
    "# from numpy import zeros\n",
    "# from scipy.linalg import svd\n",
    "# # define a matrix\n",
    "\n",
    "# # Singular-value decomposition\n",
    "# U, s, VT = svd(trans)\n",
    "# # create m x n Sigma matrix\n",
    "# Sigma = zeros((trans.shape[0], trans.shape[1]))\n",
    "# # populate Sigma with n x n diagonal matrix\n",
    "# Sigma[:trans.shape[0], :trans.shape[0]] = diag(s)\n",
    "# # select\n",
    "# n_elements = 100\n",
    "# Sigma = Sigma[:, :n_elements]\n",
    "# VT = VT[:n_elements, :]\n",
    "# # reconstruct\n",
    "# B = U.dot(Sigma.dot(VT))\n",
    "# print(B)\n",
    "# # transform\n",
    "# T = U.dot(Sigma)\n",
    "# print(T)\n",
    "# T = trans.dot(VT.T)\n",
    "# print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ad804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# define array\n",
    "\n",
    "# print(arr)\n",
    "# svd\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "svd.fit(trans)\n",
    "result = svd.transform(trans)\n",
    "print(result)\n",
    "result.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Example NumPy array\n",
    "# # numpy_array = np.result\n",
    "\n",
    "# # Convert NumPy array to DataFrame\n",
    "# dataframe = pd.DataFrame(result)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# dataframe.head(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fac067",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb1a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(category)\n",
    "print(len(category))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ecac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"D:/MCA/Sem4/LSI_Dilip/new_data_keyword.json\",'r') as f:\n",
    "#     ndata=json.load(f)\n",
    "# print(ndata)\n",
    "# list=[]\n",
    "# file=[]\n",
    "# for i in ndata.keys():\n",
    "    \n",
    "#     file.append(i)\n",
    "#     dict1=ndata.get(i)\n",
    "#     keyword=dict1.values()\n",
    "#     lst=[]\n",
    "#     for word in keyword:\n",
    "#         lst.append(word)\n",
    "#     list.append(lst)\n",
    "    \n",
    "# print(list)\n",
    "# print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4424c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # reading JSON file\n",
    "# df2 = pd.read_json(\"D:/MCA/Sem4/LSI_Dilip/new_data_keyword.json\")\n",
    "\n",
    "# # displaying sample output\n",
    "# df2 = df2.replace(np.nan, 0)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr2 = df2.to_numpy()\n",
    " \n",
    "# print('\\nNumpy Array\\n----------\\n', arr)\n",
    "# print(type(arr2))\n",
    "# arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf79621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_arr2=np.transpose(arr2)\n",
    "# print(len(trans_arr2))\n",
    "# print(trans_arr2)\n",
    "# trans_arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr3= np.concatenate((result,trans_arr2),axis=1)\n",
    "# print(arr3)\n",
    "# arr3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h=[]\n",
    "# for a in arr3:\n",
    "#     print(a)\n",
    "#     h.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6224ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y_labelencoder = LabelEncoder()\n",
    "y=y_labelencoder.fit_transform(category)\n",
    "for g in y:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b536a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(result, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90650d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071cadaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# # classifier = SVC(kernel = 'rbf', random_state=0)\n",
    "# classifier = SVC(kernel = 'rbf', gamma=0.001, C=10)\n",
    "# classifier.fit(X_train, y_train)\n",
    "# y_pred = classifier.predict(X_test) \n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# from sklearn.metrics import accuracy_score \n",
    "# print (\"Accuracy : \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b881dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# # print the classification report\n",
    "# target_names = y_labelencoder.classes_\n",
    "# print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0abb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('data.json', 'r') as f:\n",
    "#   data = json.load(f)\n",
    "# # print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8003c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    dict1=data.get(key)\n",
    "    keyword=dict1.keys()\n",
    "    print(keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c999c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip \n",
    "# import numpy as np\n",
    "# print(\"Loading pretrained Model\")\n",
    "# string=\"b'juhar'\"\n",
    "# #with open(\"catinhat.txt\") as f:\n",
    "# f=gzip.open(\"cc.bn.300.vec.gz\",'rb')\n",
    "# word_to_vec_model = dict()\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     d_word = word.decode('utf-8') #this line for decode byte object to string object\n",
    "#     coefs=np.asarray(values[1:],dtype='float32')\n",
    "#     #print(word)\n",
    "#     #embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "#     word_to_vec_model[d_word] = coefs\n",
    "# print(\"Done.\",len(word_to_vec_model),\" words loaded!\")\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008229a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "print(\"Loading pretrained Model\")\n",
    "f = gzip.open(\"cc.bn.300.vec.gz\", 'rb')\n",
    "word_to_vec_model = dict()\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0].decode('utf-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_to_vec_model[word] = coefs\n",
    "\n",
    "print(\"Done.\", len(word_to_vec_model), \" words loaded!\")\n",
    "\n",
    "f1 = open(\"data.json\")\n",
    "data = json.load(f1)\n",
    "\n",
    "document_count = len(data.keys())\n",
    "word_vector_dim = 300\n",
    "document_vectors = np.zeros((document_count, word_vector_dim))\n",
    "\n",
    "for i, document_id in enumerate(data.keys()):\n",
    "    dict1 = data.get(document_id)\n",
    "    keywords = dict1.keys()\n",
    "    word_vectors = []\n",
    "\n",
    "    for word in keywords:\n",
    "        if word in word_to_vec_model:\n",
    "            word_vectors.append(word_to_vec_model[word])\n",
    "\n",
    "    if len(word_vectors) > 0:\n",
    "        avg_vector = np.mean(word_vectors, axis=0)\n",
    "        document_vectors[i] = avg_vector\n",
    "\n",
    "f.close()\n",
    "f1.close()\n",
    "print(document_vectors)\n",
    "\n",
    "print(\"Shape of document vectors array:\", document_vectors.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db099369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(document_vectors, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa89826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# classifier = SVC(kernel = 'rbf', random_state=0)\n",
    "classifier = SVC(kernel = 'rbf', gamma=0.001, C=100)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test) \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "from sklearn.metrics import accuracy_score \n",
    "print (\"Accuracy : \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# print the classification report\n",
    "target_names = y_labelencoder.classes_\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = result + document_vectors\n",
    "print(final_result)\n",
    "final_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153fcca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(final_result, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ebd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07015eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# # classifier = SVC(kernel = 'rbf', random_state=0)\n",
    "# classifier = SVC(kernel = 'rbf', gamma=0.001, C=10)\n",
    "# classifier.fit(X_train, y_train)\n",
    "# y_pred = classifier.predict(X_test) \n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# from sklearn.metrics import accuracy_score \n",
    "# print (\"Accuracy : \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# # print the classification report\n",
    "# target_names = y_labelencoder.classes_\n",
    "# print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8d59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ccd70f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
